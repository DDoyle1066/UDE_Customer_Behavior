# Lotka-Volterra Problem
Code is based on the example [here](https://github.com/ChrisRackauckas/universal_differential_equations/blob/master/LotkaVolterra/scenario_1.jl)
Before describing the context of the problem 


This is a classical set of differential equations that describe a population of prey ``x``
and predators ``y``. Their population is determined by the following parameters: 
- α: The population growth rate of the prey
- β: The population loss rate of the prey as the predator population grows and eats them
- δ: The population growth rate of the predator as the prey population grows and more food is available
- γ: The population loss rate of the predator as the predator population 

In summary:

``\frac{dx}{dt} = αx - βxy``

``\frac{dy}{dt} = δxy - γy``
## Load packages
```julia
using OrdinaryDiffEq
using ModelingToolkit
using DataDrivenDiffEq
using DataDrivenSparse
using LinearAlgebra, ComponentArrays
using Zygote, Optimization, OptimizationOptimJL, OptimizationFlux, OptimizationOptimisers
using Lux, DiffEqFlux, SciMLSensitivity
using Plots
using Statistics
using Random
using StableRNGs
```
## True Data
Before describing the context 
```julia
rng = StableRNG(1111)
function lotka_volterra!(du, u, p, t)
    x, y = u
    α, β, δ, γ = p
    du[1] = dx = α*x - β*x*y
    du[2] = dy = δ*x*y - γ*y
end
u₀ = 5.0f0 * rand(rng, 2)
tspan = (0.0, 5.0)
p = [1.3, 0.9, 0.8, 1.8]
prob = ODEProblem(lotka_volterra!, u₀, tspan, p) 
sol = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat=0.25)

```

## Neural Network Fitting
### Data prep
```julia
X = Array(sol)
t = sol.t
x̄ = mean(X, dims =2)
ϵ = 5e-3
X_mod = X .+ (ϵ*x̄) .* randn(rng, eltype(X), size(X))

plot(sol, alpha = 0.75, color = :black, label = ["True Data" nothing])
scatter!(t, transpose(X_mod), color = :red, label = ["Noisy Data" nothing])
```
### NN setup
```julia
rbf(x) = exp.(.-(x.^2))
NN = Lux.Chain(
        Lux.Dense(2, 5, rbf), 
        Lux.Dense(5, 5, rbf), 
        Lux.Dense(5, 5, rbf),
        Lux.Dense(5, 2)
)
p_nn, st = Lux.setup(rng, NN)
# p_nn = ComponentArray(p_nn)
function vector_to_parameters(ps_new::AbstractVector, ps::NamedTuple)
    @assert length(ps_new) == Lux.parameterlength(ps)
    i = 1
    function get_ps(x)
        z = reshape(view(ps_new, i:(i + length(x) - 1)), size(x))
        i += length(x)
        return z
    end
    return fmap(get_ps, ps)
end
function ude_dynamics!(du, u, p_input, t, p_true)
    û = NN(u, vector_to_parameters(p_input, p_nn), st)[1]
    du[1] = p_true[1]*u[1] + û[1]
    du[2] = -p_true[4]*u[2] + û[2]
end
nn_dynamics!(du, u, p_nn, t) = ude_dynamics!(du, u, p_nn, t, p)
prob_nn = ODEProblem{true, SciMLBase.FullSpecialize}(nn_dynamics!, X_mod[:,1], tspan, p_nn)
function nn_predict(θ, X = X_mod[:,1], T = t)
    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)
    _sol = solve(_prob, Vern7(),
                abstol=1e-6, reltol=1e-6, saveat = T)
    return Array(_sol)
end
function loss(θ)
    X̂ = nn_predict(θ)
    sum(abs2, X̂ .- X_mod) # take log to prevent large
end
loss(ComponentArray(p_nn))
```

### NN Training
```julia
# Container to track the losses
losses = Float64[]

callback = function (p, l)
  push!(losses, l)
  if length(losses)%50==0
      println("Current loss after $(length(losses)) iterations: $(losses[end])")
  end
  return false
end
# First train with ADAM for better convergence -> move the parameters into a
# favourable starting positing for BFGS
adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)
optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p_nn))
res1 = Optimization.solve(optprob, Optimisers.ADAM(), callback = callback, maxiters = 5000, verbose = false)
println("Training loss after $(length(losses)) iterations: $(losses[end])")
# Train with BFGS
optprob2 = Optimization.OptimizationProblem(optf, res1.u)
res2 = Optimization.solve(optprob2, Optim.LBFGS(), callback = callback, maxiters = 1000)
println("Final training loss after $(length(losses)) iterations: $(losses[end])")
#println("Final training loss after $(length(losses)) iterations: $(losses[end])")
p_trained = res2.u
# Plot the losses
pl_losses = plot(1:5000, losses[1:5000], yaxis = :log10, xaxis = :log10,
                 xlabel = "Iterations", ylabel = "Loss", label = "ADAM", color = :blue)
plot!(5001:length(losses), losses[5001:end], yaxis = :log10, xaxis = :log10,
      xlabel = "Iterations", ylabel = "Loss", label = "BFGS", color = :red)
```
### Results visualization
```julia
## Analysis of the trained network
# Plot the data and the approximation
ts = first(sol.t):mean(diff(sol.t))/2:last(sol.t)
X̂ = nn_predict(p_trained, X_mod[:,1], ts)
# Trained on noisy data vs real solution
pl_trajectory = plot(ts, transpose(X̂), xlabel = "t", ylabel ="x(t), y(t)", color = :red, label = ["UDE Approximation" nothing])
scatter!(sol.t, transpose(X_mod), color = :black, label = ["Measurements" nothing])
```
```julia
# Ideal unknown interactions of the predictor
Ȳ = [-p[2]*(X̂[1,:].*X̂[2,:])';p[3]*(X̂[1,:].*X̂[2,:])']
# Neural network guess
Ŷ = NN(X̂,p_trained,st)[1]

pl_reconstruction = plot(ts, transpose(Ŷ), xlabel = "t", ylabel ="U(x,y)", color = :red, label = ["UDE Approximation" nothing])
plot!(ts, transpose(Ȳ), color = :black, label = ["True Interaction" nothing])
```
```julia
# Plot the error
pl_reconstruction_error = plot(ts, norm.(eachcol(Ȳ - Ŷ)), yaxis = :log, xlabel = "t",
                               ylabel = "L2-Error", label = nothing, color = :red)
pl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2, 1))

pl_overall = plot(pl_trajectory, pl_missing)
```
## Structural Inference
```julia
# Create a Basis
@variables u[1:2]
# Generate the basis functions, multivariate polynomials up to deg 5
# and sine
b = polynomial_basis(u, 4)
basis = Basis(b,u)
# Create the thresholds which should be used in the search process
λ = exp10.(-3:0.01:3)
# Create an optimizer for the SINDy problem
opt = ADMM(λ)
# Define different problems for the recovery
full_problem = ContinuousDataDrivenProblem(X_mod, t)
ideal_problem = DirectDataDrivenProblem(X̂, Ȳ)
nn_problem = DirectDataDrivenProblem(X̂, Ŷ)
options = DataDrivenCommonOptions(maxiters = 10_000,
                                  normalize = DataNormalization(ZScoreTransform),
                                  selector = bic,digits = 1,
                                  data_processing = DataProcessing(split = 0.9,
                                                                   batchsize = 30,
                                                                   shuffle = true,
                                                                   rng = StableRNG(1111)))
# Test on ideal derivative data for unknown function ( not available )
println("Sparse regression")
full_res = solve(full_problem, basis, opt, options = options)
full_eqs = get_basis(full_res)
println(full_eqs)
ideal_res = solve(ideal_problem, basis, opt, options = options)
ideal_eqs = get_basis(ideal_res)
println(ideal_eqs)
nn_res = solve(nn_problem, basis, opt, options = options)
nn_eqs = get_basis(nn_res)
println(nn_eqs)
for eqs in (full_eqs, ideal_eqs, nn_eqs)
    println(eqs)
    println(get_parameter_map(eqs))
    println()
end
```

```julia
# Define the recovered, hybrid model
function recovered_dynamics!(du, u, p_est, t)
    û = nn_eqs(u, p_est) # Recovered equations
    du[1] = p[1] * u[1] + û[1]
    du[2] = -p[4] * u[2] + û[2]
end

estimation_prob = ODEProblem(recovered_dynamics!, u₀, tspan, get_parameter_values(nn_eqs))
estimate = solve(estimation_prob, Tsit5(), saveat = t)

# Plot
plot(sol)
plot!(estimate)
```
```julia
function parameter_loss(p)
    Y = reduce(hcat, map(Base.Fix2(nn_eqs, p), eachcol(X̂)))
    sum(abs2, Ŷ .- Y)
end

optf = Optimization.OptimizationFunction((x, p) -> parameter_loss(x), adtype)
optprob = Optimization.OptimizationProblem(optf, get_parameter_values(nn_eqs))
parameter_res = Optimization.solve(optprob, Optim.LBFGS(), maxiters = 1000)
```


```julia
# Look at long term prediction
t_long = (0.0, 50.0)
prob_nn_long = ODEProblem(nn_dynamics!, u₀, t_long, p_trained)
estimate_nn_long = solve(prob_nn_long, Tsit5(), abstol = 1e-6, reltol=1e-6, saveat = 0.1) # Using higher tolerances here results in exit of julia
plot(estimate_nn_long, label = ["Approx u1(t)" "Approx u2(t)"])

true_prob = ODEProblem(lotka_volterra!, u₀, t_long, p)
true_solution_long = solve(true_prob, Tsit5(), abstol = 1e-6, reltol=1e-6, saveat = estimate_nn_long.t)
plot!(true_solution_long, label = ["True u1(t)" "True u2(t)"])
```
```julia
# Look at long term prediction
t_long = (0.0, 50.0)
estimation_prob = ODEProblem(recovered_dynamics!, u₀, t_long, parameter_res)
estimate_long = solve(estimation_prob, Tsit5(), abstol = 1e-6, reltol=1e-6, saveat = 0.1) # Using higher tolerances here results in exit of julia
plot(estimate_long, label = ["Approx u1(t)" "Approx u2(t)"])

true_prob = ODEProblem(lotka_volterra!, u₀, t_long, p)
true_solution_long = solve(true_prob, Tsit5(), abstol = 1e-6, reltol=1e-6, saveat = estimate_long.t)
plot!(true_solution_long, label = ["True u1(t)" "True u2(t)"])
```
## Bayesian Network Fitting
```julia
using Turing
alpha = 0.09
sig = sqrt(1 /alpha)
function nn_predict_turing(params, layers)
    # 
end
@model function deep_bayes_ode(data)
    σ ~ InverseGamma(2,3)
    n_params = Lux.parameterlength(p_nn)
    nn_params ~  MvNormal(zeros(n_params), sig .* ones(n_params))
    preds = nn_predict(nn_params)
    for i in 1:size(data)[2]
        data[:,i] ~ MvNormal(preds[:,i], σ)
    end
    return nothing
end
```
```julia
deep_bayes_ode_model = deep_bayes_ode(X_mod)
chain = sample(deep_bayes_ode_model, NUTS(0.65), 1000)
```
## Probabilistic Structure Inference